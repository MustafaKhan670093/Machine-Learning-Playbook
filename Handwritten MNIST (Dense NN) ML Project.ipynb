{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the training and test data\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "#Download and load the testing data\n",
    "valset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the ML model\n",
    "\n",
    "#The following is a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2958, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our log-probabilities\n",
    "logps = model(images)\n",
    "# Calculate the loss with the logps and the labels\n",
    "loss = criterion(logps, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6424, -0.9220],\n",
      "        [-1.3359, -1.4522]], requires_grad=True)\n",
      "tensor([[0.4127, 0.8501],\n",
      "        [1.7847, 2.1089]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)\n",
    "y = x**2\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x7f8cade19510>\n",
      "tensor(1.2891, grad_fn=<MeanBackward0>)\n",
      "tensor([[-0.3212, -0.4610],\n",
      "        [-0.6680, -0.7261]])\n",
      "tensor([[-0.3212, -0.4610],\n",
      "        [-0.6680, -0.7261]], grad_fn=<DivBackward0>)\n",
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[-0.0040, -0.0040, -0.0040,  ..., -0.0040, -0.0040, -0.0040],\n",
      "        [ 0.0029,  0.0029,  0.0029,  ...,  0.0029,  0.0029,  0.0029],\n",
      "        [ 0.0012,  0.0012,  0.0012,  ...,  0.0012,  0.0012,  0.0012],\n",
      "        ...,\n",
      "        [ 0.0037,  0.0037,  0.0037,  ...,  0.0037,  0.0037,  0.0037],\n",
      "        [-0.0016, -0.0016, -0.0016,  ..., -0.0016, -0.0016, -0.0016],\n",
      "        [ 0.0015,  0.0015,  0.0015,  ...,  0.0015,  0.0015,  0.0015]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## grad_fn shows the function that generated this variable\n",
    "print(y.grad_fn)\n",
    "\n",
    "z = y.mean()\n",
    "print(z)\n",
    "\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)\n",
    "\n",
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[ 0.0035,  0.0270,  0.0229,  ..., -0.0095, -0.0067,  0.0350],\n",
      "        [ 0.0051,  0.0339,  0.0253,  ...,  0.0284,  0.0196, -0.0317],\n",
      "        [ 0.0096, -0.0027, -0.0205,  ...,  0.0025,  0.0115,  0.0127],\n",
      "        ...,\n",
      "        [-0.0032,  0.0348, -0.0302,  ..., -0.0339,  0.0119,  0.0105],\n",
      "        [-0.0222,  0.0038, -0.0149,  ..., -0.0027, -0.0134,  0.0126],\n",
      "        [-0.0178,  0.0009,  0.0144,  ...,  0.0271, -0.0045,  0.0028]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[ 0.0004,  0.0004,  0.0004,  ...,  0.0004,  0.0004,  0.0004],\n",
      "        [ 0.0009,  0.0009,  0.0009,  ...,  0.0009,  0.0009,  0.0009],\n",
      "        [ 0.0010,  0.0010,  0.0010,  ...,  0.0010,  0.0010,  0.0010],\n",
      "        ...,\n",
      "        [ 0.0017,  0.0017,  0.0017,  ...,  0.0017,  0.0017,  0.0017],\n",
      "        [ 0.0004,  0.0004,  0.0004,  ...,  0.0004,  0.0004,  0.0004],\n",
      "        [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]])\n",
      "Updated weights -  Parameter containing:\n",
      "tensor([[ 0.0035,  0.0270,  0.0229,  ..., -0.0095, -0.0067,  0.0350],\n",
      "        [ 0.0051,  0.0339,  0.0253,  ...,  0.0284,  0.0196, -0.0317],\n",
      "        [ 0.0096, -0.0027, -0.0205,  ...,  0.0025,  0.0115,  0.0127],\n",
      "        ...,\n",
      "        [-0.0032,  0.0348, -0.0302,  ..., -0.0340,  0.0119,  0.0105],\n",
      "        [-0.0222,  0.0038, -0.0149,  ..., -0.0027, -0.0134,  0.0126],\n",
      "        [-0.0178,  0.0009,  0.0144,  ...,  0.0271, -0.0045,  0.0028]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#TRAINING THE MODEL (using pytorch's optim package. For example we can use stochastic gradient descent with optim.SGD)\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "print('Initial weights - ', model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model[0].weight.grad)\n",
    "\n",
    "# Take an update step and few the new weights\n",
    "optimizer.step()\n",
    "print('Updated weights - ', model[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.35365282442333346\n",
      "Training loss: 0.33616056063694993\n",
      "Training loss: 0.32315398997335293\n",
      "Training loss: 0.31302414152191393\n",
      "Training loss: 0.30369639862924497\n"
     ]
    }
   ],
   "source": [
    "#TRAINING FOR REAL (Now we'll put this algorithm into a loop so we can go through all the images.\n",
    "# For each batch, we'll doing a training pass where we calculate the loss, do a backwards pass, and update the weights.)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "\n",
    "        #TODO: Training pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss / len(trainloader)}\")\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the image\n",
    "\n",
    "def view_classify(img, ps, version=\"MNIST\"):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    if version == \"MNIST\":\n",
    "        ax2.set_yticklabels(np.arange(10))\n",
    "    elif version == \"Fashion\":\n",
    "        ax2.set_yticklabels(['T-shirt/top',\n",
    "                            'Trouser',\n",
    "                            'Pullover',\n",
    "                            'Dress',\n",
    "                            'Coat',\n",
    "                            'Sandal',\n",
    "                            'Shirt',\n",
    "                            'Sneaker',\n",
    "                            'Bag',\n",
    "                            'Ankle Boot'], size='small');\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()     #Note, in pycharm you need to add plt.show() to view results.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_classify(img.view(1, 28, 28), ps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.9155\n"
     ]
    }
   ],
   "source": [
    "#Determining Accuracy of Model\n",
    "\n",
    "correct_count, all_count = 0, 0\n",
    "for images, labels in valloader:\n",
    "    for i in range(len(labels)):\n",
    "        img = images[i].view(1, 784)\n",
    "        with torch.no_grad():\n",
    "            logps = model(img)\n",
    "\n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        pred_label = probab.index(max(probab))\n",
    "        true_label = labels.numpy()[i]\n",
    "        if (true_label == pred_label):\n",
    "            correct_count += 1\n",
    "        all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count / all_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
