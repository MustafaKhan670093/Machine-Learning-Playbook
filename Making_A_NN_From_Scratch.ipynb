{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Making A NN From Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOm09RYI9l3bKESf5hEhKaK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MustafaKhan670093/Machine-Learning-Playbook/blob/master/Making_A_NN_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHUiiz-xMhCK"
      },
      "source": [
        "# **Making A NN From Scratch By Mustafa** \n",
        "\n",
        "As part of an effort to study machine learning and continue making what I am calling the Machine Learning Playbook, I have constructed a NN from the mathematical understanding I have of how NNs work.\n",
        "\n",
        "**Acknowledgement:** [Aditya](http://adityamehrotra.ca/), a friend of mine, was kind enough to walk me through his [writeup](https://github.com/AditMeh/deep-learning/blob/main/backpropagation_notes.pdf) on the mathematics behind NNs and explain key concepts such as backpropagation. My work is based on the conversation I had as well as his writeup." 
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bdu7gcWBgIT"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUv7NEL-ME_8"
      },
      "source": [
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEfw1vYWBh1e"
      },
      "source": [
        "## **Neural Network Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWsX6bkKMn6S"
      },
      "source": [
        "class NeuralNet:\n",
        "    def __init__(self, layer_list):\n",
        "      #layer list structure = [1, 2, 3] \n",
        "      self.layer_list = layer_list\n",
        "      self.weights = []                    # weights = [ [[1, 2, 3], [1, 2, 3]], [[1, 2, 3], [1, 2, 3]]]\n",
        "      self.bias = [] \n",
        "      \n",
        "      #Initialize random weights and biases\n",
        "      for i in range(1, len(layer_list)):\n",
        "        rows = layer_list[i]\n",
        "        cols = layer_list[i-1]\n",
        "        self.weights.append(np.random.rand(rows,cols)*0.01)\n",
        "        self.bias.append(np.random.rand(rows,1)*0.01)\n",
        "      \n",
        "      print(\"Weights\")\n",
        "      print([element.shape for element in self.weights])\n",
        "\n",
        "      print(\"\\n\" + \"Biases\")\n",
        "      print([element.shape for element in self.bias])\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "      sigmoid = 1/(1+(np.e)**-x)\n",
        "      return sigmoid\n",
        "    \n",
        "    def derivative_sigmoid(self, x):\n",
        "      derivative = self.sigmoid(x)*(1-self.sigmoid(x))\n",
        "      return derivative\n",
        "    \n",
        "    def cost(self, a, y):\n",
        "      cost = 0.5*((a-y)**2)\n",
        "      cost = np.sum(cost)\n",
        "      return cost\n",
        "    \n",
        "    def derivative_cost(self, a, y):\n",
        "      derivative = a - y\n",
        "      return derivative\n",
        "    \n",
        "    def feed_forward(self, x, y):\n",
        "      self.y = y\n",
        "      self.x = x\n",
        "      self.z = []\n",
        "      self.activation = [x] #Note: Activation of first layer is just the inputs x\n",
        "      \n",
        "      for i in range(len(self.weights)):\n",
        "        z_value = np.dot(self.weights[i], np.array(self.activation[i])) + np.array(self.bias[i])\n",
        "        self.activation.append(self.sigmoid(z_value))\n",
        "        self.z.append(z_value)\n",
        "      \n",
        "      cost = self.cost(self.activation[-1], y)\n",
        "\n",
        "      return cost\n",
        "      \n",
        "    def gradients_and_biases(self):\n",
        "      grad_weights = [np.zeros(element.shape) for element in self.weights]\n",
        "      grad_bias = [np.zeros(element.shape) for element in self.bias]\n",
        "      delta = self.derivative_cost(self.activation[-1], self.y) * self.derivative_sigmoid(self.z[-1]) #Delta of final layer\n",
        "      derivative_cost_wrt_weight = np.dot(delta, (self.activation[-2]).T)\n",
        "      derivative_cost_wrt_bias = delta\n",
        "      grad_weights[-1] = derivative_cost_wrt_weight\n",
        "      grad_bias[-1] = derivative_cost_wrt_bias\n",
        "\n",
        "      for i in range(2, len(self.layer_list)):\n",
        "        delta = np.dot((self.weights[-i+1].T), delta) * self.derivative_sigmoid(self.z[-i])\n",
        "        derivative_cost_wrt_weight = np.dot(delta, (self.activation[-i-1]).T)\n",
        "        derivative_cost_wrt_bias = delta\n",
        "        grad_weights[-i] = derivative_cost_wrt_weight\n",
        "        grad_bias[-i] = derivative_cost_wrt_bias\n",
        "\n",
        "      return grad_weights, grad_bias\n",
        "\n",
        "    def update_weights_and_biases(self, alpha, grad_weights, grad_bias):\n",
        "      for i in range(len(self.weights)):\n",
        "        self.weights[i] = self.weights[i] - alpha*grad_weights[i]\n",
        "        self.bias[i] = self.bias[i] - alpha*grad_bias[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAcD2fobBl8Z"
      },
      "source": [
        "## **Testing And Debugging With Random Inputs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTXT6eoOUL1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df36aa9b-e99d-4575-f794-feba1ef79fea"
      },
      "source": [
        "nn = NeuralNet([1, 2, 3])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights\n",
            "[(2, 1), (3, 2)]\n",
            "\n",
            "Biases\n",
            "[(2, 1), (3, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU31IjKnUbY2"
      },
      "source": [
        "nn = NeuralNet([1, 2, 3])\n",
        "test_input = np.random.rand(1,1)\n",
        "test_output = np.random.rand(3,1)\n",
        "\n",
        "print(\"Cost: \" + str(nn.feed_forward(test_input, test_output)))\n",
        "print(\"\\n\" + \"Activations\")\n",
        "print([element.shape for element in nn.activation])\n",
        "print(\"\\n\" + \"Z\")\n",
        "print([element.shape for element in nn.z])\n",
        "print(nn.gradients_and_biases())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fVqCVybBtSR"
      },
      "source": [
        "## **Testing NN On MNIST**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eDR_sRXC-qf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ff12fefe-3a90-4328-8a4f-706e7fec500d"
      },
      "source": [
        "#Importing dataset\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "#Preparing train and test loaders\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2]) / 255\n",
        "\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
        "\n",
        "#Preparing batches of data\n",
        "def prepare_mini_batches(minibatch_size):\n",
        "    random_indexes = []\n",
        "    indexes = np.random.choice(X_train.shape[0], minibatch_size, replace=False)\n",
        "    random_indexes.append(indexes) \n",
        "    return random_indexes\n",
        "\n",
        "#One hot encoding classes\n",
        "def one_hot_encode(length, index):\n",
        "    output = [0 for i in range(length)]\n",
        "    output[index] = 1\n",
        "    return np.asarray(output).reshape(length, 1)\n",
        "\n",
        "#Computing Losses\n",
        "def compute_test_loss(test_set, net):\n",
        "    total_loss = []\n",
        "    for i in range(len(X_test)):\n",
        "        loss = net.feed_forward(X_test[i].reshape(X_test[i].shape[0], 1), one_hot_encode(10 ,Y_test[i]))\n",
        "        total_loss.append(loss)\n",
        "    total = 0\n",
        "    for i in total_loss:\n",
        "      total += i\n",
        "    return total/len(total_loss)\n",
        "\n",
        "#Instantiate neural net\n",
        "nn = NeuralNet([784, 392, 196, 10])\n",
        "epochs = 200\n",
        "batch_size = 32\n",
        "\n",
        "#Training loop\n",
        "for i in range(epochs):\n",
        "  random_indexes = prepare_mini_batches(batch_size)\n",
        "  for mini_batch in random_indexes:\n",
        "    grad_weights = [np.zeros(element.shape) for element in nn.weights]\n",
        "    grad_bias = [np.zeros(element.shape) for element in nn.bias]\n",
        "    for j in mini_batch:\n",
        "       #X_train[j].reshape(X_train[j].shape[0], 1), one_hot_encode(10 ,Y_train[j])\n",
        "       nn.feed_forward(X_train[j].reshape(X_train[j].shape[0], 1), one_hot_encode(10 ,Y_train[j]))\n",
        "       grad_w, grad_b = nn.gradients_and_biases()\n",
        "       for k in range(len(grad_w)):\n",
        "         grad_weights[k] = grad_weights[k] + grad_w[k]\n",
        "         grad_bias[k] = grad_bias[k] + grad_b[k]\n",
        "    grad_weights = [grad_weights[i]/batch_size for i in range(len(grad_weights))]    #The act of averaging the grads is Stochastic Gradient Descent\n",
        "    grad_bias = [grad_bias[i]/batch_size for i in range(len(grad_bias))]\n",
        "    nn.update_weights_and_biases(0.003, grad_weights, grad_bias)\n",
        "  \n",
        "  print(\"Updating Weights\")\n",
        "  print(\"Test Loss = \" + str(compute_test_loss(X_test, nn)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights\n",
            "[(392, 784), (196, 392), (10, 196)]\n",
            "\n",
            "Biases\n",
            "[(392, 1), (196, 1), (10, 1)]\n",
            "Updating Weights\n",
            "Test Loss = 2.2261691985990826\n",
            "Updating Weights\n",
            "Test Loss = 2.161878003356405\n",
            "Updating Weights\n",
            "Test Loss = 2.09783092495534\n",
            "Updating Weights\n",
            "Test Loss = 2.0342344608202603\n",
            "Updating Weights\n",
            "Test Loss = 1.9714416279101865\n",
            "Updating Weights\n",
            "Test Loss = 1.9092243037490628\n",
            "Updating Weights\n",
            "Test Loss = 1.848054056982265\n",
            "Updating Weights\n",
            "Test Loss = 1.7877886750352139\n",
            "Updating Weights\n",
            "Test Loss = 1.728666903839946\n",
            "Updating Weights\n",
            "Test Loss = 1.6713015724997384\n",
            "Updating Weights\n",
            "Test Loss = 1.6154801395965606\n",
            "Updating Weights\n",
            "Test Loss = 1.5613988007351536\n",
            "Updating Weights\n",
            "Test Loss = 1.5091317644669577\n",
            "Updating Weights\n",
            "Test Loss = 1.4584800279923946\n",
            "Updating Weights\n",
            "Test Loss = 1.4100592527218416\n",
            "Updating Weights\n",
            "Test Loss = 1.3634576635046556\n",
            "Updating Weights\n",
            "Test Loss = 1.3186840424233146\n",
            "Updating Weights\n",
            "Test Loss = 1.275882343275331\n",
            "Updating Weights\n",
            "Test Loss = 1.234963987578975\n",
            "Updating Weights\n",
            "Test Loss = 1.1957291442215303\n",
            "Updating Weights\n",
            "Test Loss = 1.15881396566258\n",
            "Updating Weights\n",
            "Test Loss = 1.1235086996328054\n",
            "Updating Weights\n",
            "Test Loss = 1.0899499013995564\n",
            "Updating Weights\n",
            "Test Loss = 1.0582339557529494\n",
            "Updating Weights\n",
            "Test Loss = 1.0281875162610694\n",
            "Updating Weights\n",
            "Test Loss = 0.9997382943415043\n",
            "Updating Weights\n",
            "Test Loss = 0.9729457145116136\n",
            "Updating Weights\n",
            "Test Loss = 0.947499008765185\n",
            "Updating Weights\n",
            "Test Loss = 0.9233069463312853\n",
            "Updating Weights\n",
            "Test Loss = 0.9006059869476292\n",
            "Updating Weights\n",
            "Test Loss = 0.8790910703124455\n",
            "Updating Weights\n",
            "Test Loss = 0.8587743060022133\n",
            "Updating Weights\n",
            "Test Loss = 0.8395325873645996\n",
            "Updating Weights\n",
            "Test Loss = 0.8213626446322\n",
            "Updating Weights\n",
            "Test Loss = 0.8042440105843248\n",
            "Updating Weights\n",
            "Test Loss = 0.7880082228076605\n",
            "Updating Weights\n",
            "Test Loss = 0.7726328020440106\n",
            "Updating Weights\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-4680d3d31b63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Updating Weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Loss = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_test_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-4680d3d31b63>\u001b[0m in \u001b[0;36mcompute_test_loss\u001b[0;34m(test_set, net)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-79d4ed1578ae>\u001b[0m in \u001b[0;36mfeed_forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mz_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgRGmnJeCIoX"
      },
      "source": [
        "I interrupted the training since it would run for 200 epochs. Neverthless, it appears as if we have been successful in our implementation of the neural network!\n"
      ]
    }
  ]
}
